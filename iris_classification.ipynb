{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n lo·∫°i hoa Di√™n vƒ© (Iris) v·ªõi Machine Learning\n",
    "\n",
    "## 1. Gi·ªõi thi·ªáu\n",
    "\n",
    "Notebook n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n x√¢y d·ª±ng m·ªôt m√¥ h√¨nh Machine Learning ƒë·ªÉ ph√¢n lo·∫°i ba lo√†i hoa di√™n vƒ© (Iris) d·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng v·∫≠t l√Ω. ƒê√¢y l√† m·ªôt b√†i to√°n ph√¢n lo·∫°i c·ªï ƒëi·ªÉn v√† l√† ƒëi·ªÉm kh·ªüi ƒë·∫ßu tuy·ªát v·ªùi ƒë·ªÉ h·ªçc Machine Learning.\n",
    "\n",
    "**M·ª•c ti√™u:**\n",
    "- Kh√°m ph√° v√† hi·ªÉu d·ªØ li·ªáu Iris\n",
    "- Th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "- Hu·∫•n luy·ªán v√† so s√°nh nhi·ªÅu m√¥ h√¨nh ML\n",
    "- T·ªëi ∆∞u h√≥a m√¥ h√¨nh t·ªët nh·∫•t\n",
    "- ƒê√°nh gi√° v√† l∆∞u m√¥ h√¨nh cu·ªëi c√πng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Th∆∞ vi·ªán tr·ª±c quan h√≥a\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Thi·∫øt l·∫≠p style cho bi·ªÉu ƒë·ªì\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Thi·∫øt l·∫≠p hi·ªÉn th·ªã\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# T·∫Øt c·∫£nh b√°o\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán c·∫ßn thi·∫øt!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th∆∞ vi·ªán Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# C√°c thu·∫≠t to√°n ML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh\n",
    "import joblib\n",
    "\n",
    "print(\"‚úÖ ƒê√£ import th√†nh c√¥ng t·∫•t c·∫£ th∆∞ vi·ªán Machine Learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T·∫£i v√† kh√°m ph√° d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "print(\"üìä Th√¥ng tin c∆° b·∫£n v·ªÅ d·ªØ li·ªáu:\")\n",
    "print(f\"K√≠ch th∆∞·ªõc d·ªØ li·ªáu: {df.shape[0]} m·∫´u, {df.shape[1]} ƒë·∫∑c tr∆∞ng\")\n",
    "print(\"\\nüìã 5 d√≤ng ƒë·∫ßu ti√™n:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th√¥ng tin chi ti·∫øt v·ªÅ d·ªØ li·ªáu\n",
    "print(\"üìä Th√¥ng tin chi ti·∫øt v·ªÅ dataset:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nüìà Th·ªëng k√™ m√¥ t·∫£:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra gi√° tr·ªã thi·∫øu\n",
    "print(\"üîç Ki·ªÉm tra gi√° tr·ªã thi·∫øu:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ Kh√¥ng c√≥ gi√° tr·ªã thi·∫øu n√†o trong d·ªØ li·ªáu!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è C√≥ gi√° tr·ªã thi·∫øu c·∫ßn x·ª≠ l√Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ph√¢n b·ªë c√°c lo√†i hoa\n",
    "print(\"üå∏ Ph√¢n b·ªë c√°c lo√†i hoa:\")\n",
    "species_counts = df['species'].value_counts()\n",
    "print(species_counts)\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë\n",
    "plt.figure(figsize=(8, 6))\n",
    "species_counts.plot(kind='bar', color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "plt.title('Ph√¢n b·ªë c√°c lo√†i hoa Iris', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Lo√†i hoa')\n",
    "plt.ylabel('S·ªë l∆∞·ª£ng m·∫´u')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ D·ªØ li·ªáu c√¢n b·∫±ng ho√†n h·∫£o: m·ªói lo√†i c√≥ {species_counts.iloc[0]} m·∫´u\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kh√°m ph√° d·ªØ li·ªáu (EDA - Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi t·ª´ng ƒë·∫∑c tr∆∞ng\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "feature_names = ['Chi·ªÅu d√†i ƒë√†i hoa', 'Chi·ªÅu r·ªông ƒë√†i hoa', 'Chi·ªÅu d√†i c√°nh hoa', 'Chi·ªÅu r·ªông c√°nh hoa']\n",
    "\n",
    "for i, (feature, name) in enumerate(zip(features, feature_names)):\n",
    "    row, col = i // 2, i % 2\n",
    "    axes[row, col].hist(df[feature], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[row, col].set_title(f'Ph√¢n ph·ªëi {name}', fontweight='bold')\n",
    "    axes[row, col].set_xlabel(name)\n",
    "    axes[row, col].set_ylabel('T·∫ßn su·∫•t')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Ph√¢n ph·ªëi c√°c ƒë·∫∑c tr∆∞ng c·ªßa hoa Iris', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot theo t·ª´ng lo√†i\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, (feature, name) in enumerate(zip(features, feature_names)):\n",
    "    row, col = i // 2, i % 2\n",
    "    sns.boxplot(data=df, x='species', y=feature, ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{name} theo lo√†i', fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Lo√†i hoa')\n",
    "    axes[row, col].set_ylabel(name)\n",
    "\n",
    "plt.suptitle('So s√°nh c√°c ƒë·∫∑c tr∆∞ng gi·ªØa c√°c lo√†i hoa', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot - So s√°nh t·∫•t c·∫£ c√°c c·∫∑p ƒë·∫∑c tr∆∞ng\n",
    "plt.figure(figsize=(12, 10))\n",
    "pairplot = sns.pairplot(df, hue='species', diag_kind='hist', \n",
    "                       plot_kws={'alpha': 0.7, 's': 50})\n",
    "pairplot.fig.suptitle('Ma tr·∫≠n so s√°nh c√°c ƒë·∫∑c tr∆∞ng theo lo√†i', \n",
    "                      fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Nh·∫≠n x√©t t·ª´ Pair Plot:\")\n",
    "print(\"- Setosa r·∫•t d·ªÖ ph√¢n bi·ªát v·ªõi hai lo√†i c√≤n l·∫°i\")\n",
    "print(\"- Versicolor v√† Virginica c√≥ s·ª± ch·ªìng ch√©o nh∆∞ng v·∫´n c√≥ th·ªÉ ph√¢n bi·ªát\")\n",
    "print(\"- Chi·ªÅu d√†i v√† r·ªông c√°nh hoa l√† hai ƒë·∫∑c tr∆∞ng ph√¢n bi·ªát t·ªët nh·∫•t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ma tr·∫≠n t∆∞∆°ng quan\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Ma tr·∫≠n t∆∞∆°ng quan gi·ªØa c√°c ƒë·∫∑c tr∆∞ng', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Ph√¢n t√≠ch t∆∞∆°ng quan:\")\n",
    "print(\"- Chi·ªÅu d√†i c√°nh hoa v√† chi·ªÅu r·ªông c√°nh hoa c√≥ t∆∞∆°ng quan m·∫°nh (r=0.963)\")\n",
    "print(\"- Chi·ªÅu d√†i ƒë√†i hoa v√† chi·ªÅu d√†i c√°nh hoa c√≥ t∆∞∆°ng quan kh√° cao (r=0.872)\")\n",
    "print(\"- Chi·ªÅu r·ªông ƒë√†i hoa c√≥ t∆∞∆°ng quan √¢m v·ªõi c√°c ƒë·∫∑c tr∆∞ng kh√°c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√°ch ƒë·∫∑c tr∆∞ng v√† nh√£n\n",
    "X = df[features]  # C√°c ƒë·∫∑c tr∆∞ng\n",
    "y = df['species']  # Nh√£n (lo√†i hoa)\n",
    "\n",
    "print(\"üéØ K√≠ch th∆∞·ªõc d·ªØ li·ªáu:\")\n",
    "print(f\"X (ƒë·∫∑c tr∆∞ng): {X.shape}\")\n",
    "print(f\"y (nh√£n): {y.shape}\")\n",
    "\n",
    "print(\"\\nüìä Th·ªëng k√™ X:\")\n",
    "print(X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üìä K√≠ch th∆∞·ªõc sau khi chia:\")\n",
    "print(f\"T·∫≠p hu·∫•n luy·ªán - X: {X_train.shape}, y: {y_train.shape}\")\n",
    "print(f\"T·∫≠p ki·ªÉm tra - X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nüå∏ Ph√¢n b·ªë lo√†i trong t·∫≠p hu·∫•n luy·ªán:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nüå∏ Ph√¢n b·ªë lo√†i trong t·∫≠p ki·ªÉm tra:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚öñÔ∏è So s√°nh tr∆∞·ªõc v√† sau khi chu·∫©n h√≥a:\")\n",
    "print(\"\\nTr∆∞·ªõc chu·∫©n h√≥a (X_train):\")\n",
    "print(pd.DataFrame(X_train).describe())\n",
    "\n",
    "print(\"\\nSau chu·∫©n h√≥a (X_train_scaled):\")\n",
    "print(pd.DataFrame(X_train_scaled, columns=features).describe())\n",
    "\n",
    "print(\"\\n‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a v·ªõi mean‚âà0 v√† std‚âà1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hu·∫•n luy·ªán m√¥ h√¨nh baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o c√°c m√¥ h√¨nh\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"ü§ñ C√°c m√¥ h√¨nh s·∫Ω ƒë∆∞·ª£c hu·∫•n luy·ªán:\")\n",
    "for name in models.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hu·∫•n luy·ªán v√† ƒë√°nh gi√° c√°c m√¥ h√¨nh\n",
    "results = []\n",
    "\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"ƒêang hu·∫•n luy·ªán {name}...\")\n",
    "    \n",
    "    # C√°c m√¥ h√¨nh c·∫ßn chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "    if name in ['Logistic Regression', 'K-Nearest Neighbors', 'Support Vector Machine']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    \n",
    "    # T√≠nh ƒë·ªô ch√≠nh x√°c\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': accuracy,\n",
    "        'CV Mean': cv_scores.mean(),\n",
    "        'CV Std': cv_scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ {name}: Test Accuracy = {accuracy:.3f}, CV = {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "\n",
    "print(\"\\nüéâ Ho√†n th√†nh hu·∫•n luy·ªán t·∫•t c·∫£ m√¥ h√¨nh!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªïng h·ª£p k·∫øt qu·∫£\n",
    "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
    "\n",
    "print(\"üìä B·∫£ng t·ªïng h·ª£p k·∫øt qu·∫£:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = np.arange(len(results_df))\n",
    "\n",
    "plt.bar(x_pos, results_df['Test Accuracy'], alpha=0.7, color='lightblue', \n",
    "        edgecolor='navy', label='Test Accuracy')\n",
    "plt.bar(x_pos, results_df['CV Mean'], alpha=0.7, color='lightcoral', \n",
    "        edgecolor='darkred', label='CV Mean', width=0.6)\n",
    "\n",
    "plt.xlabel('M√¥ h√¨nh')\n",
    "plt.ylabel('ƒê·ªô ch√≠nh x√°c')\n",
    "plt.title('So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh Machine Learning', fontweight='bold')\n",
    "plt.xticks(x_pos, results_df['Model'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_accuracy = results_df.iloc[0]['Test Accuracy']\n",
    "print(f\"\\nüèÜ M√¥ h√¨nh t·ªët nh·∫•t: {best_model_name} v·ªõi ƒë·ªô ch√≠nh x√°c {best_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. T·ªëi ∆∞u si√™u tham s·ªë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªëi ∆∞u cho Support Vector Machine (th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët)\n",
    "print(\"üîß T·ªëi ∆∞u si√™u tham s·ªë cho Support Vector Machine...\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a l∆∞·ªõi tham s·ªë\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "    'kernel': ['rbf', 'linear', 'poly']\n",
    "}\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh SVC\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "# Grid Search v·ªõi Cross Validation\n",
    "grid_search = GridSearchCV(\n",
    "    svc, param_grid, cv=5, scoring='accuracy', \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Hu·∫•n luy·ªán v·ªõi d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n th√†nh t·ªëi ∆∞u si√™u tham s·ªë!\")\n",
    "print(f\"üèÜ Tham s·ªë t·ªët nh·∫•t: {grid_search.best_params_}\")\n",
    "print(f\"üìä ƒêi·ªÉm s·ªë CV t·ªët nh·∫•t: {grid_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªëi ∆∞u cho Random Forest\n",
    "print(\"üîß T·ªëi ∆∞u si√™u tham s·ªë cho Random Forest...\")\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid_search_rf = GridSearchCV(\n",
    "    rf, param_grid_rf, cv=5, scoring='accuracy', \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Hu·∫•n luy·ªán v·ªõi d·ªØ li·ªáu ch∆∞a chu·∫©n h√≥a (Random Forest kh√¥ng c·∫ßn)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úÖ Ho√†n th√†nh t·ªëi ∆∞u Random Forest!\")\n",
    "print(f\"üèÜ Tham s·ªë t·ªët nh·∫•t: {grid_search_rf.best_params_}\")\n",
    "print(f\"üìä ƒêi·ªÉm s·ªë CV t·ªët nh·∫•t: {grid_search_rf.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So s√°nh m√¥ h√¨nh t·ªëi ∆∞u\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# ƒê√°nh gi√° tr√™n t·∫≠p test\n",
    "svc_pred = best_svc.predict(X_test_scaled)\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "\n",
    "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(\"üèÜ K·∫øt qu·∫£ sau t·ªëi ∆∞u:\")\n",
    "print(f\"SVC t·ªëi ∆∞u: {svc_accuracy:.3f}\")\n",
    "print(f\"Random Forest t·ªëi ∆∞u: {rf_accuracy:.3f}\")\n",
    "\n",
    "# Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t\n",
    "if svc_accuracy >= rf_accuracy:\n",
    "    final_model = best_svc\n",
    "    final_model_name = \"Support Vector Machine\"\n",
    "    final_accuracy = svc_accuracy\n",
    "    X_test_final = X_test_scaled\n",
    "    y_pred_final = svc_pred\n",
    "else:\n",
    "    final_model = best_rf\n",
    "    final_model_name = \"Random Forest\"\n",
    "    final_accuracy = rf_accuracy\n",
    "    X_test_final = X_test\n",
    "    y_pred_final = rf_pred\n",
    "\n",
    "print(f\"\\nüéØ M√¥ h√¨nh cu·ªëi c√πng: {final_model_name} v·ªõi ƒë·ªô ch√≠nh x√°c {final_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ƒê√°nh gi√° chi ti·∫øt tr√™n t·∫≠p test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B√°o c√°o ph√¢n lo·∫°i chi ti·∫øt\n",
    "print(f\"üìä B√°o c√°o ƒë√°nh gi√° chi ti·∫øt cho {final_model_name}:\")\n",
    "print(\"=\" * 60)\n",
    "report = classification_report(y_test, y_pred_final)\n",
    "print(report)\n",
    "\n",
    "# ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ\n",
    "print(f\"\\nüéØ ƒê·ªô ch√≠nh x√°c t·ªïng th·ªÉ: {final_accuracy:.3f} ({final_accuracy*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "species_names = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=species_names, yticklabels=species_names,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title(f'Ma tr·∫≠n nh·∫ßm l·∫´n - {final_model_name}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('D·ª± ƒëo√°n')\n",
    "plt.ylabel('Th·ª±c t·∫ø')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ph√¢n t√≠ch ma tr·∫≠n nh·∫ßm l·∫´n\n",
    "print(\"üîç Ph√¢n t√≠ch ma tr·∫≠n nh·∫ßm l·∫´n:\")\n",
    "for i, species in enumerate(species_names):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i, :].sum()\n",
    "    accuracy_species = correct / total if total > 0 else 0\n",
    "    print(f\"- {species.capitalize()}: {correct}/{total} ({accuracy_species:.1%})\")\n",
    "\n",
    "# T√¨m l·ªói ph√¢n lo·∫°i\n",
    "errors = y_test != y_pred_final\n",
    "if errors.sum() > 0:\n",
    "    print(f\"\\n‚ùå C√≥ {errors.sum()} l·ªói ph√¢n lo·∫°i:\")\n",
    "    error_df = pd.DataFrame({\n",
    "        'Th·ª±c t·∫ø': y_test[errors],\n",
    "        'D·ª± ƒëo√°n': y_pred_final[errors]\n",
    "    })\n",
    "    print(error_df.to_string())\n",
    "else:\n",
    "    print(\"\\n‚úÖ Kh√¥ng c√≥ l·ªói ph√¢n lo·∫°i n√†o! M√¥ h√¨nh ho√†n h·∫£o!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L∆∞u m√¥ h√¨nh v√† suy lu·∫≠n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u m√¥ h√¨nh t·ªët nh·∫•t\n",
    "model_filename = 'iris_model.joblib'\n",
    "scaler_filename = 'iris_scaler.joblib'\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh v√†o {model_filename}\")\n",
    "\n",
    "# L∆∞u scaler n·∫øu c·∫ßn\n",
    "if final_model_name == \"Support Vector Machine\":\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u scaler v√†o {scaler_filename}\")\n",
    "\n",
    "print(f\"\\nüì¶ M√¥ h√¨nh {final_model_name} ƒë√£ s·∫µn s√†ng ƒë·ªÉ s·ª≠ d·ª•ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• suy lu·∫≠n v·ªõi d·ªØ li·ªáu m·ªõi\n",
    "print(\"üîÆ V√≠ d·ª• suy lu·∫≠n v·ªõi d·ªØ li·ªáu m·ªõi:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# T·∫°o m·ªôt s·ªë m·∫´u th·ªß c√¥ng ƒë·ªÉ test\n",
    "new_samples = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Gi·ªëng setosa ƒëi·ªÉn h√¨nh\n",
    "    [6.0, 2.7, 5.1, 1.6],  # Gi·ªëng versicolor ƒëi·ªÉn h√¨nh  \n",
    "    [7.2, 3.2, 6.0, 1.8],  # Gi·ªëng virginica ƒëi·ªÉn h√¨nh\n",
    "    [5.0, 3.0, 4.0, 1.0],  # M·∫´u trung gian\n",
    "])\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu cho d·ª± ƒëo√°n\n",
    "if final_model_name == \"Support Vector Machine\":\n",
    "    new_samples_processed = scaler.transform(new_samples)\n",
    "else:\n",
    "    new_samples_processed = new_samples\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "predictions = final_model.predict(new_samples_processed)\n",
    "probabilities = final_model.predict_proba(new_samples_processed) if hasattr(final_model, 'predict_proba') else None\n",
    "\n",
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "feature_names_vn = ['D√†i ƒë√†i', 'R·ªông ƒë√†i', 'D√†i c√°nh', 'R·ªông c√°nh']\n",
    "\n",
    "for i, (sample, pred) in enumerate(zip(new_samples, predictions)):\n",
    "    print(f\"\\nM·∫´u {i+1}:\")\n",
    "    for feature, value in zip(feature_names_vn, sample):\n",
    "        print(f\"  {feature}: {value}\")\n",
    "    print(f\"  üå∏ D·ª± ƒëo√°n: {pred.capitalize()}\")\n",
    "    \n",
    "    if probabilities is not None:\n",
    "        prob_dict = dict(zip(final_model.classes_, probabilities[i]))\n",
    "        print(f\"  üìä X√°c su·∫•t:\")\n",
    "        for species, prob in prob_dict.items():\n",
    "            print(f\"    - {species.capitalize()}: {prob:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra load l·∫°i m√¥ h√¨nh\n",
    "print(\"üîÑ Ki·ªÉm tra t·∫£i l·∫°i m√¥ h√¨nh...\")\n",
    "\n",
    "# Load m√¥ h√¨nh\n",
    "loaded_model = joblib.load(model_filename)\n",
    "if final_model_name == \"Support Vector Machine\":\n",
    "    loaded_scaler = joblib.load(scaler_filename)\n",
    "\n",
    "# Test v·ªõi m·ªôt m·∫´u\n",
    "test_sample = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "if final_model_name == \"Support Vector Machine\":\n",
    "    test_sample_processed = loaded_scaler.transform(test_sample)\n",
    "else:\n",
    "    test_sample_processed = test_sample\n",
    "\n",
    "loaded_prediction = loaded_model.predict(test_sample_processed)\n",
    "print(f\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c t·∫£i v√† ho·∫°t ƒë·ªông: {loaded_prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. K·∫øt lu·∫≠n v√† h∆∞·ªõng ph√°t tri·ªÉn\n",
    "\n",
    "### üéâ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:\n",
    "\n",
    "1. **X√¢y d·ª±ng th√†nh c√¥ng** m√¥ h√¨nh ph√¢n lo·∫°i hoa Iris v·ªõi ƒë·ªô ch√≠nh x√°c cao\n",
    "2. **So s√°nh nhi·ªÅu thu·∫≠t to√°n** ML kh√°c nhau v√† ch·ªçn ra m√¥ h√¨nh t·ªët nh·∫•t\n",
    "3. **T·ªëi ∆∞u si√™u tham s·ªë** ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t\n",
    "4. **ƒê√°nh gi√° chi ti·∫øt** v·ªõi confusion matrix v√† classification report\n",
    "5. **L∆∞u m√¥ h√¨nh** ƒë·ªÉ s·ª≠ d·ª•ng trong t∆∞∆°ng lai\n",
    "\n",
    "### üìä Nh·ªØng ƒëi·ªÅu h·ªçc ƒë∆∞·ª£c:\n",
    "\n",
    "- **EDA r·∫•t quan tr·ªçng**: Gi√∫p hi·ªÉu d·ªØ li·ªáu v√† l·ª±a ch·ªçn chi·∫øn l∆∞·ª£c ph√π h·ª£p\n",
    "- **Chu·∫©n h√≥a d·ªØ li·ªáu**: C·∫ßn thi·∫øt cho m·ªôt s·ªë thu·∫≠t to√°n (SVM, KNN, Logistic Regression)\n",
    "- **Cross-validation**: ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa m√¥ h√¨nh\n",
    "- **Grid search**: T·ª± ƒë·ªông t√¨m si√™u tham s·ªë t·ªët nh·∫•t\n",
    "\n",
    "### üöÄ H∆∞·ªõng ph√°t tri·ªÉn:\n",
    "\n",
    "1. **Th·ª≠ nghi·ªám th√™m m√¥ h√¨nh**:\n",
    "   - Neural Networks\n",
    "   - Gradient Boosting (XGBoost, LightGBM)\n",
    "   - Ensemble methods\n",
    "\n",
    "2. **K·ªπ thu·∫≠t ti·ªÅn x·ª≠ l√Ω n√¢ng cao**:\n",
    "   - Principal Component Analysis (PCA)\n",
    "   - Feature selection\n",
    "   - Polynomial features\n",
    "\n",
    "3. **T·ªëi ∆∞u h√≥a n√¢ng cao**:\n",
    "   - RandomizedSearchCV\n",
    "   - Bayesian Optimization (Optuna)\n",
    "   - AutoML tools\n",
    "\n",
    "4. **Tri·ªÉn khai th·ª±c t·∫ø**:\n",
    "   - T·∫°o web app v·ªõi Streamlit/Flask\n",
    "   - API v·ªõi FastAPI\n",
    "   - Containerize v·ªõi Docker\n",
    "   - Deploy l√™n cloud\n",
    "\n",
    "5. **Monitoring v√† MLOps**:\n",
    "   - Model versioning v·ªõi MLflow\n",
    "   - Continuous training\n",
    "   - A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "### üåü L·ªùi k·∫øt:\n",
    "\n",
    "B√†i to√°n ph√¢n lo·∫°i hoa Iris tuy ƒë∆°n gi·∫£n nh∆∞ng l√† n·ªÅn t·∫£ng tuy·ªát v·ªùi ƒë·ªÉ h·ªçc Machine Learning. T·ª´ ƒë√¢y, b·∫°n c√≥ th·ªÉ √°p d·ª•ng ki·∫øn th·ª©c n√†y v√†o c√°c b√†i to√°n ph·ª©c t·∫°p h∆°n trong th·ª±c t·∫ø!\n",
    "\n",
    "**Ch√∫c m·ª´ng b·∫°n ƒë√£ ho√†n th√†nh d·ª± √°n Machine Learning ƒë·∫ßu ti√™n! üéä**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}